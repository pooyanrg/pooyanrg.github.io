---
title: "Research"
collection: teaching
permalink: /research/
---

## Research Interests: 

My research interest is mainly focused on Explainable AI (XAI) in both Computer Vision (CV) and Natural Language Processing (NLP). Previously, I focused on using Reinforcement Learning (RL) methods in Robotic platforms. My goal was to help robots acquire complex skills via Deep RL.

## Research To Date:

As part of my Ph.D. research, I am working on self-explainable and editable models for downstream CV/NLP tasks. More specifically, my research centers on the interpretability of minimal transformer layers, which we call attention bottlenecks.

## Proposed  Research 
In the era of Transformers, renowned for their efficacy in vision and language tasks, the challenge of explainability becomes very important. These models, while powerful, often operate as black boxes, hindering comprehension by human users. Recognizing this, I propose the integration of self-explainable models to demystify their inner workings. To this end, I aim to incorporate an attention bottleneck module within Transformers. This proposed bottleneck not only enhances the explainability of these models but also facilitates their editability, thus fostering greater transparency and usability.


