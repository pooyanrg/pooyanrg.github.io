---
title: "Research"
collection: teaching
permalink: /research/
---

## Research Interests: 

* Explainable AI
  + Interpretable Transformers for CV and NLP tasks
  + How to Use an Attention Bottleneck for Transformer Interpretability
* Multi-Modal Models and their Applications
  + Image Difference Prediction
  + Image Editing
 
During my Master's, I worked on Reinforcement Learning (RL) and Robotics, where my goal was to help robotic platforms acquire complex skills via RL.


## Research To Date:

As part of my Ph.D. research, I am working on self-explainable and editable models for downstream CV/NLP tasks. More specifically, my research centers on the interpretability of minimal transformer layers (attention bottlenecks).

## Proposed  Research

In the era of Transformers, renowned for their efficacy in vision and language tasks, the challenge of explainability becomes very important. These models, while powerful, often operate as black boxes, hindering comprehension by human users. Recognizing this, I propose integrating self-explainable models to demystify the inner workings of transformers. To this end, I aim to incorporate an attention bottleneck module within transformers. This proposed bottleneck enhances the explainability of these models and facilitates their editability, thus fostering greater transparency and usability.


