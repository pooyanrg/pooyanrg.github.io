---
title: "Vision language models are blind"
collection: publications
permalink: /publication/accv_2024
excerpt: ''
date: 2024-09-08
venue: 'Proceedings of the Asian Conference on Computer Vision'
paperurl: 'https://link.springer.com/chapter/10.1007/978-981-96-0917-8_17'
citation: 'Rahmanzadehgervi, P., Bolton, L., Taesiri, M.R. and Nguyen, A.T., 2024. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision (pp. 18-34).'
---
Large language models (LLMs) with vision capabilities (e.g., GPT-4o, Gemini 1.5, and Claude 3) are powering countless image-text processing applications, enabling unprecedented multimodal, human-machine interaction. Yet, we find that all state-of-the-art LLMs fail on absurdly simple tasks such as identifying (a) whether two circles overlap or whether two lines touch each other; (b) which letter is being circled in a word; and (c) counting the number of circles in a Olympic-like logo. Our findings suggest the tokenization of input images to LLMs is the source of problem, causing failures in real-world scenarios, such as determining if two streets intersect on a Manhattan map, identifying a stock price crossing a threshold line, and describing content within a bounding box in an image. 

[Download](https://doi.org/10.1007/978-981-96-0917-8_17)
